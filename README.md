# CrossScale-ECG-A-Modular-HPC-Pipeline-from-Locality-Optimization-to-MPI-GPU-Overlap
We introduce three coordinated modules that expose bottlenecks in biomedical FL. The first module isolates data locality as the primary variable, restructuring access patterns, enabling page-locked buffers, and issuing non-blocking transfers. Module 2 incorporates OpenMP-driven CPU parallelism and Module 3 exploits GPU stream overlap and AMP.

Federated learning (FL) offers a privacy-preserving approach for collaborative training of biomedical machine learning (ML) models. However, FL introduces new performance bottlenecks across heterogeneous hardware and distributed execution. While prior work focuses on privacy, communication, and high-level model performance, low-level system behavior remains largely unexplored in biomedical workloads. This study presents the first end-to-end, HPC-oriented performance characterization of a federated ECG classification pipeline using the MIT-BIH Arrhythmia Database. We evaluate three cross-stack modules: (1) CPU/GPU data-movement optimizations, where contiguous sampling and pinned memory with non-blocking transfers improve H2D latency and increase GPU throughput by 15–25\%; (2) hand-optimized CPU 1D convolution kernels using OpenMP and AVX2, achieving $2\times$–$3.5\times$ speedup over PyTorch’s native implementation depending on batch size and kernel width; and (3) GPU compute/communication overlap, MPI-based aggregation, and automatic mixed precision (AMP) in realistic multi-node FL, demonstrating that communication increasingly dominates runtime at larger world sizes, but local optimizations still yield substantial gains. Our results show that classical HPC techniques, often neglected in biomedical ML, significantly reduce memory, compute, and communication bottlenecks, underscoring the importance of cross-stack systems-level co-design for scalable, privacy-preserving biomedical FL pipelines.

Reference the respective README.md in each of the modules for usage instructions.
